{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59a47fbd-54a0-4303-9930-bec21ee4af13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7b02256-6f46-482a-aca7-58f371706d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory to the Python path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62dbc040-c179-4893-8bca-7c556d757394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download raw data from 2023 to 2024\n",
      "File already exists for 2023-01.\n",
      "Loading data for 2023-01...\n",
      "Total records: 3,066,766\n",
      "Valid records: 2,993,140\n",
      "Records dropped: 73,626 (2.40%)\n",
      "Successfully processed data for 2023-01.\n",
      "File already exists for 2023-02.\n",
      "Loading data for 2023-02...\n",
      "Total records: 2,913,955\n",
      "Valid records: 2,845,058\n",
      "Records dropped: 68,897 (2.36%)\n",
      "Successfully processed data for 2023-02.\n",
      "File already exists for 2023-03.\n",
      "Loading data for 2023-03...\n",
      "Total records: 3,403,766\n",
      "Valid records: 3,331,705\n",
      "Records dropped: 72,061 (2.12%)\n",
      "Successfully processed data for 2023-03.\n",
      "File already exists for 2023-04.\n",
      "Loading data for 2023-04...\n",
      "Total records: 3,288,250\n",
      "Valid records: 3,214,922\n",
      "Records dropped: 73,328 (2.23%)\n",
      "Successfully processed data for 2023-04.\n",
      "File already exists for 2023-05.\n",
      "Loading data for 2023-05...\n",
      "Total records: 3,513,649\n",
      "Valid records: 3,435,875\n",
      "Records dropped: 77,774 (2.21%)\n",
      "Successfully processed data for 2023-05.\n",
      "File already exists for 2023-06.\n",
      "Loading data for 2023-06...\n",
      "Total records: 3,307,234\n",
      "Valid records: 3,233,969\n",
      "Records dropped: 73,265 (2.22%)\n",
      "Successfully processed data for 2023-06.\n",
      "File already exists for 2023-07.\n",
      "Loading data for 2023-07...\n",
      "Total records: 2,907,108\n",
      "Valid records: 2,838,637\n",
      "Records dropped: 68,471 (2.36%)\n",
      "Successfully processed data for 2023-07.\n",
      "File already exists for 2023-08.\n",
      "Loading data for 2023-08...\n",
      "Total records: 2,824,209\n",
      "Valid records: 2,758,739\n",
      "Records dropped: 65,470 (2.32%)\n",
      "Successfully processed data for 2023-08.\n",
      "File already exists for 2023-09.\n",
      "Loading data for 2023-09...\n",
      "Total records: 2,846,722\n",
      "Valid records: 2,782,920\n",
      "Records dropped: 63,802 (2.24%)\n",
      "Successfully processed data for 2023-09.\n",
      "File already exists for 2023-10.\n",
      "Loading data for 2023-10...\n",
      "Total records: 3,522,285\n",
      "Valid records: 3,446,406\n",
      "Records dropped: 75,879 (2.15%)\n",
      "Successfully processed data for 2023-10.\n",
      "File already exists for 2023-11.\n",
      "Loading data for 2023-11...\n",
      "Total records: 3,339,715\n",
      "Valid records: 3,267,940\n",
      "Records dropped: 71,775 (2.15%)\n",
      "Successfully processed data for 2023-11.\n",
      "File already exists for 2023-12.\n",
      "Loading data for 2023-12...\n",
      "Total records: 3,376,567\n",
      "Valid records: 3,313,957\n",
      "Records dropped: 62,610 (1.85%)\n",
      "Successfully processed data for 2023-12.\n",
      "Combining all monthly data...\n",
      "Data loading and processing complete!\n",
      "Data loading complete.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from src.data_utils import load_and_process_taxi_data\n",
    "\n",
    "from_year = 2023\n",
    "# to_year = datetime.now().year\n",
    "to_year = 2024\n",
    "print(f\"Download raw data from {from_year} to {to_year}\")\n",
    "\n",
    "rides = pd.DataFrame()\n",
    "chunks = []\n",
    "for year in range(from_year, to_year+1):\n",
    "\n",
    "    rides_one_year = load_and_process_taxi_data(year)\n",
    "\n",
    "    chunks.append(rides_one_year)\n",
    "    break\n",
    "\n",
    "# Concatenate all chunks at the end\n",
    "rides = pd.concat(chunks, ignore_index=True)\n",
    "print(\"Data loading complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce5589a2-9316-4dac-ba2e-e9a8a4789241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>pickup_location_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-01 00:32:10</td>\n",
       "      <td>161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-01-01 00:55:08</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-01-01 00:25:04</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-01-01 00:03:48</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-01-01 00:10:29</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37463263</th>\n",
       "      <td>2023-12-31 23:04:34</td>\n",
       "      <td>233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37463264</th>\n",
       "      <td>2023-12-31 23:08:15</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37463265</th>\n",
       "      <td>2023-12-31 23:16:15</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37463266</th>\n",
       "      <td>2023-12-31 23:21:58</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37463267</th>\n",
       "      <td>2023-12-31 23:10:47</td>\n",
       "      <td>237</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37463268 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             pickup_datetime  pickup_location_id\n",
       "0        2023-01-01 00:32:10                 161\n",
       "1        2023-01-01 00:55:08                  43\n",
       "2        2023-01-01 00:25:04                  48\n",
       "3        2023-01-01 00:03:48                 138\n",
       "4        2023-01-01 00:10:29                 107\n",
       "...                      ...                 ...\n",
       "37463263 2023-12-31 23:04:34                 233\n",
       "37463264 2023-12-31 23:08:15                  48\n",
       "37463265 2023-12-31 23:16:15                 196\n",
       "37463266 2023-12-31 23:21:58                 140\n",
       "37463267 2023-12-31 23:10:47                 237\n",
       "\n",
       "[37463268 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59f47e90-0fcb-4b7a-80d0-a68c979f9ea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37463268, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rides.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5259f4bd-65ce-43dc-b487-09ee12d964bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_utils import transform_raw_data_into_ts_data\n",
    "\n",
    "ts_data = transform_raw_data_into_ts_data(rides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59252b5f-17fc-4207-a1c4-6bebd49233ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2277600, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92402d0c-dacd-4039-ba40-caf7eeafe8b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2277600 entries, 0 to 2277599\n",
      "Data columns (total 3 columns):\n",
      " #   Column              Dtype         \n",
      "---  ------              -----         \n",
      " 0   pickup_hour         datetime64[ns]\n",
      " 1   pickup_location_id  int16         \n",
      " 2   rides               int16         \n",
      "dtypes: datetime64[ns](1), int16(2)\n",
      "memory usage: 26.1 MB\n"
     ]
    }
   ],
   "source": [
    "ts_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3804e864-fcde-41f4-bf92-5c0e8e90aa0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_hour</th>\n",
       "      <th>pickup_location_id</th>\n",
       "      <th>rides</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-01 00:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-01-01 01:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-01-01 02:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-01-01 03:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-01-01 04:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2277595</th>\n",
       "      <td>2023-12-31 19:00:00</td>\n",
       "      <td>263</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2277596</th>\n",
       "      <td>2023-12-31 20:00:00</td>\n",
       "      <td>263</td>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2277597</th>\n",
       "      <td>2023-12-31 21:00:00</td>\n",
       "      <td>263</td>\n",
       "      <td>232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2277598</th>\n",
       "      <td>2023-12-31 22:00:00</td>\n",
       "      <td>263</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2277599</th>\n",
       "      <td>2023-12-31 23:00:00</td>\n",
       "      <td>263</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2277600 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                pickup_hour  pickup_location_id  rides\n",
       "0       2023-01-01 00:00:00                   2      0\n",
       "1       2023-01-01 01:00:00                   2      0\n",
       "2       2023-01-01 02:00:00                   2      0\n",
       "3       2023-01-01 03:00:00                   2      0\n",
       "4       2023-01-01 04:00:00                   2      0\n",
       "...                     ...                 ...    ...\n",
       "2277595 2023-12-31 19:00:00                 263    188\n",
       "2277596 2023-12-31 20:00:00                 263    198\n",
       "2277597 2023-12-31 21:00:00                 263    232\n",
       "2277598 2023-12-31 22:00:00                 263    160\n",
       "2277599 2023-12-31 23:00:00                 263     95\n",
       "\n",
       "[2277600 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8949a20a-f5da-45ff-990f-d789d234cdf6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-04 18:36:22,478 INFO: Initializing external client\n",
      "2025-03-04 18:36:22,479 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "2025-03-04 18:36:23,138 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1214695\n",
      "Successfully connected to Hopsworks project: sp25_taxi_nmuppala\n"
     ]
    }
   ],
   "source": [
    "import hopsworks\n",
    "\n",
    "api_key = os.getenv('HOPSWORKS_API_KEY')  \n",
    "project_name = os.getenv('HOPSWORKS_PROJECT_NAME')  \n",
    "\n",
    "# pip install confluent-kafka\n",
    "# Initialize connection to Hopsworks  \n",
    "project = hopsworks.login(  \n",
    "    api_key_value=api_key,  \n",
    "    project=project_name  \n",
    ")  \n",
    "print(f\"Successfully connected to Hopsworks project: {project_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae5a3cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_store = project.get_feature_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4294fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_GROUP_NAME = \"time_series_hourly_feature_group\"\n",
    "FEATURE_GROUP_VERSION = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f885dc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_group = feature_store.get_or_create_feature_group(\n",
    "    name=FEATURE_GROUP_NAME,\n",
    "    version=FEATURE_GROUP_VERSION,\n",
    "    primary_key=['pickup_location_id', 'pickup_hour'],\n",
    "    event_time=['pickup_hour']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e05e91ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Group created successfully, explore it at \n",
      "https://c.app.hopsworks.ai:443/p/1214695/fs/1202330/fg/1402015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |██████████| Rows 2277600/2277600 | Elapsed Time: 00:54 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: time_series_hourly_feature_group_1_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://c.app.hopsworks.ai:443/p/1214695/jobs/named/time_series_hourly_feature_group_1_offline_fg_materialization/executions\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Job('time_series_hourly_feature_group_1_offline_fg_materialization', 'SPARK'),\n",
       " None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_group.insert(ts_data, write_options={\"wait_for_job\": False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c360d4a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature group created successfully!\n"
     ]
    }
   ],
   "source": [
    "from hsfs.feature_group import FeatureGroup\n",
    "\n",
    "feature_store = project.get_feature_store()\n",
    "\n",
    "# Create the feature group\n",
    "feature_group = feature_store.create_feature_group(\n",
    "    name=\"time_series_hourly_feature_group\",\n",
    "    version=1,\n",
    "    description=\"Feature group for hourly time series data\",\n",
    "    primary_key=[\"timestamp\"],  # Adjust primary key according to your dataset\n",
    "    online_enabled=True\n",
    ")\n",
    "print(\"Feature group created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fbf8a538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature group created with supported primary key for online serving.\n"
     ]
    }
   ],
   "source": [
    "feature_group = feature_store.create_feature_group(\n",
    "    name=\"time_series_hourly_feature_group\",\n",
    "    version=1,\n",
    "    description=\"Feature group of hourly time series data\",  # Use the new 'id' column as the primary key\n",
    "    online_enabled=True   # Enable online serving if needed\n",
    ")\n",
    "print(\"Feature group created with supported primary key for online serving.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "28a35b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_data['pickup_hour'] = pd.to_datetime(ts_data['pickup_hour'])\n",
    "ts_data['pickup_location_id'] = ts_data['pickup_location_id'].astype(int)\n",
    "ts_data['rides'] = ts_data['rides'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "23970890",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_GROUP_NAME = \"time_series_hourly_feature_group\"\n",
    "FEATURE_GROUP_VERSION = 1\n",
    "\n",
    "feature_group = feature_store.get_or_create_feature_group(\n",
    "    name=FEATURE_GROUP_NAME,\n",
    "    version=FEATURE_GROUP_VERSION,\n",
    "    description=\"Time series data at hourly frequency\",\n",
    "    primary_key=['pickup_location_id', 'pickup_hour'],\n",
    "    event_time=['pickup_hour']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a2a1268d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Group created successfully, explore it at \n",
      "https://c.app.hopsworks.ai:443/p/1214695/fs/1202330/fg/1401999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |██████████| Rows 2277600/2277600 | Elapsed Time: 00:55 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: time_series_hourly_feature_group_1_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://c.app.hopsworks.ai:443/p/1214695/jobs/named/time_series_hourly_feature_group_1_offline_fg_materialization/executions\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Job('time_series_hourly_feature_group_1_offline_fg_materialization', 'SPARK'),\n",
       " None)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_data.rename(columns={\n",
    "    \"pickup_hour\": 'pickup_hour',\n",
    "    \"pickup_location_id\": 'pickup_location_id',\n",
    "    \"rides\": 'rides'\n",
    "}, inplace=True)\n",
    "\n",
    "ts_data_selected = ts_data[['pickup_hour', 'pickup_location_id', 'rides']]\n",
    "\n",
    "feature_group.insert(ts_data_selected, write_options={\"wait_for_job\": False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e23ce4f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame size: 857.47 MB\n"
     ]
    }
   ],
   "source": [
    "df_memory_mb = rides.memory_usage(deep=True).sum() / (1024 * 1024)  \n",
    "print(f\"DataFrame size: {df_memory_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fa5badd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2277600 entries, 0 to 2277599\n",
      "Data columns (total 3 columns):\n",
      " #   Column              Dtype         \n",
      "---  ------              -----         \n",
      " 0   pickup_hour         datetime64[ns]\n",
      " 1   pickup_location_id  int64         \n",
      " 2   rides               int64         \n",
      "dtypes: datetime64[ns](1), int64(2)\n",
      "memory usage: 52.1 MB\n"
     ]
    }
   ],
   "source": [
    "ts_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "348b1a4b-6141-4078-bc9f-e9e3a750a0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_store = project.get_feature_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d54682e-b7d3-49fc-86bd-65b1b06206bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_GROUP_NAME = \"time_series_hourly_feature_group\"\n",
    "FEATURE_GROUP_VERSION = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f7dd4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature group created successfully!\n"
     ]
    }
   ],
   "source": [
    "from hsfs.feature_group import FeatureGroup\n",
    "\n",
    "# Create the feature group\n",
    "feature_group = feature_store.create_feature_group(\n",
    "    name=\"time_series_hourly_feature_group\",\n",
    "    version=1,\n",
    "    description=\"Feature group for hourly time series data\",\n",
    "    primary_key=[\"timestamp\"],  # Adjust primary key according to your dataset\n",
    "    online_enabled=True\n",
    ")\n",
    "print(\"Feature group created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9bb71b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature group created with supported primary key for online serving.\n"
     ]
    }
   ],
   "source": [
    "feature_group = feature_store.create_feature_group(\n",
    "    name=\"time_series_hourly_feature_group\",\n",
    "    version=1,\n",
    "    description=\"Feature group for hourly time series data\",  # Use the new 'id' column as the primary key\n",
    "    online_enabled=True   # Enable online serving if needed\n",
    ")\n",
    "print(\"Feature group created with supported primary key for online serving.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "08928326",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_data.rename(columns={\n",
    "    'pickup_hour': 'pickup_hour',\n",
    "    'pickup_location_id': 'pickup_location_id',\n",
    "    'rides': 'rides'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f537e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_data['pickup_hour'] = pd.to_datetime(ts_data['pickup_hour'])\n",
    "ts_data['pickup_location_id'] = ts_data['pickup_location_id'].astype(int)\n",
    "ts_data['rides'] = ts_data['rides'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "be1383db",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_GROUP_NAME = \"time_series_hourly_feature_group\"\n",
    "FEATURE_GROUP_VERSION = 1\n",
    "\n",
    "feature_group = feature_store.get_or_create_feature_group(\n",
    "    name=FEATURE_GROUP_NAME,\n",
    "    version=FEATURE_GROUP_VERSION,\n",
    "    description=\"Time series data at hourly frequency\",\n",
    "    primary_key=['pickup_location_id', 'pickup_hour'],\n",
    "    event_time=['pickup_hour']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "46a7333c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_data = ts_data.rename(columns={\n",
    "    \"pickup_hour\": \"timestamp\",\n",
    "    \"rides\": \"ride_count\",\n",
    "    \"pickup_location_id\": \"pickup_location\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "46ee5a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Group created successfully, explore it at \n",
      "https://c.app.hopsworks.ai:443/p/1214695/fs/1202330/fg/1401992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |██████████| Rows 2277600/2277600 | Elapsed Time: 00:55 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: time_series_hourly_feature_group_1_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://c.app.hopsworks.ai:443/p/1214695/jobs/named/time_series_hourly_feature_group_1_offline_fg_materialization/executions\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Job('time_series_hourly_feature_group_1_offline_fg_materialization', 'SPARK'),\n",
       " None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rename the columns\n",
    "ts_data.rename(columns={\n",
    "    'timestamp': 'pickup_hour',\n",
    "    'pickup_location': 'pickup_location_id',\n",
    "    'ride_count': 'rides'\n",
    "}, inplace=True)\n",
    "\n",
    "# Select only the required columns\n",
    "ts_data_selected = ts_data[['pickup_hour', 'pickup_location_id', 'rides']]\n",
    "\n",
    "# Insert the selected columns into the feature group\n",
    "feature_group.insert(ts_data_selected, write_options={\"wait_for_job\": False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2f0ced7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame size: 857.47 MB\n"
     ]
    }
   ],
   "source": [
    "df_memory_mb = rides.memory_usage(deep=True).sum() / (1024 * 1024)  \n",
    "print(f\"DataFrame size: {df_memory_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a3fa0df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2277600 entries, 0 to 2277599\n",
      "Data columns (total 3 columns):\n",
      " #   Column              Dtype         \n",
      "---  ------              -----         \n",
      " 0   pickup_hour         datetime64[ns]\n",
      " 1   pickup_location_id  int64         \n",
      " 2   rides               int64         \n",
      "dtypes: datetime64[ns](1), int64(2)\n",
      "memory usage: 52.1 MB\n"
     ]
    }
   ],
   "source": [
    "ts_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906317bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f47e80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8d90ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                 pickup_hour  pickup_location_id  rides      pickup_hour_str\n",
       "0       2023-01-01 00:00:00                   2      0  2023-01-01 00:00:00\n",
       "1       2023-01-01 01:00:00                   2      0  2023-01-01 01:00:00\n",
       "2       2023-01-01 02:00:00                   2      0  2023-01-01 02:00:00\n",
       "3       2023-01-01 03:00:00                   2      0  2023-01-01 03:00:00\n",
       "4       2023-01-01 04:00:00                   2      0  2023-01-01 04:00:00\n",
       "...                     ...                 ...    ...                  ...\n",
       "2277595 2023-12-31 19:00:00                 263    188  2023-12-31 19:00:00\n",
       "2277596 2023-12-31 20:00:00                 263    198  2023-12-31 20:00:00\n",
       "2277597 2023-12-31 21:00:00                 263    232  2023-12-31 21:00:00\n",
       "2277598 2023-12-31 22:00:00                 263    160  2023-12-31 22:00:00\n",
       "2277599 2023-12-31 23:00:00                 263     95  2023-12-31 23:00:00\n",
       "\n",
       "[2277600 rows x 4 columns]>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_data['pickup_hour_str'] = ts_data['pickup_hour'].dt.strftime('%Y-%m-%d %H:00:00')\n",
    "ts_data['pickup_hour'] = pd.to_datetime(ts_data['pickup_hour'])\n",
    "\n",
    "\n",
    "ts_data.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "eeab5c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2277600 entries, 0 to 2277599\n",
      "Data columns (total 4 columns):\n",
      " #   Column              Dtype         \n",
      "---  ------              -----         \n",
      " 0   pickup_hour         datetime64[ns]\n",
      " 1   pickup_location_id  int16         \n",
      " 2   rides               int16         \n",
      " 3   pickup_hour_str     object        \n",
      "dtypes: datetime64[ns](1), int16(2), object(1)\n",
      "memory usage: 43.4+ MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ts_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f292eed-c4a1-4d3a-8ee9-daa7bb1fea5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_group = feature_store.get_or_create_feature_group(\n",
    "    name=\"time_series_hourly_feature_group\",\n",
    "    version=1,\n",
    "    description=\"Feature group for ride data\",\n",
    "    primary_key=[\"pickup_hour\", \"pickup_location_id\"],\n",
    "    event_time=\"pickup_hour\",\n",
    "    online_enabled=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bda251e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pickup_hour           datetime64[ns]\n",
      "pickup_location_id             int16\n",
      "rides                          int16\n",
      "dtype: object\n",
      "Index(['pickup_hour', 'pickup_location_id', 'rides'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(ts_data.dtypes)\n",
    "print(ts_data.columns)\n",
    "\n",
    "feature_group.schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ec573dd2-3125-4d2f-93a9-975bedfe739f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Group created successfully, explore it at \n",
      "https://c.app.hopsworks.ai:443/p/1214695/fs/1202330/fg/1401983\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Producer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfeature_group\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mts_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwrite_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwait_for_job\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_proj_cda500_env/lib/python3.10/site-packages/hsfs/feature_group.py:2959\u001b[0m, in \u001b[0;36minsert\u001b[0;34m(self, features, overwrite, operation, storage, write_options, validation_options, wait, transformation_context, transform)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_proj_cda500_env/lib/python3.10/site-packages/hsfs/core/feature_group_engine.py:224\u001b[0m, in \u001b[0;36minsert\u001b[0;34m(self, feature_group, feature_dataframe, overwrite, operation, storage, write_options, validation_options, transformation_context, transform)\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feature_group_api\u001b[38;5;241m.\u001b[39mdelete_content(feature_group)\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    212\u001b[0m         engine\u001b[38;5;241m.\u001b[39mget_instance()\u001b[38;5;241m.\u001b[39msave_dataframe(\n\u001b[1;32m    213\u001b[0m             feature_group,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    221\u001b[0m         ge_report,\n\u001b[1;32m    222\u001b[0m     )\n\u001b[0;32m--> 224\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdelete\u001b[39m(\u001b[38;5;28mself\u001b[39m, feature_group):\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feature_group_api\u001b[38;5;241m.\u001b[39mdelete(feature_group)\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcommit_details\u001b[39m(\u001b[38;5;28mself\u001b[39m, feature_group, wallclock_time, limit):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_proj_cda500_env/lib/python3.10/site-packages/hsfs/engine/python.py:815\u001b[0m, in \u001b[0;36mEngine.save_dataframe\u001b[0;34m(self, feature_group, dataframe, operation, online_enabled, storage, offline_write_options, online_write_options, validation_id)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_dataframe\u001b[39m(\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    802\u001b[0m     feature_group: FeatureGroup,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    809\u001b[0m     validation_id: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    810\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[job\u001b[38;5;241m.\u001b[39mJob]:\n\u001b[1;32m    811\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;28mhasattr\u001b[39m(feature_group, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEXTERNAL_FEATURE_GROUP\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m feature_group\u001b[38;5;241m.\u001b[39monline_enabled\n\u001b[1;32m    814\u001b[0m     ) \u001b[38;5;129;01mor\u001b[39;00m feature_group\u001b[38;5;241m.\u001b[39mstream:\n\u001b[0;32m--> 815\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_write_dataframe_kafka\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    816\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfeature_group\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffline_write_options\u001b[49m\n\u001b[1;32m    817\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    818\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    819\u001b[0m         \u001b[38;5;66;03m# for backwards compatibility\u001b[39;00m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlegacy_save_dataframe(\n\u001b[1;32m    821\u001b[0m             feature_group,\n\u001b[1;32m    822\u001b[0m             dataframe,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    828\u001b[0m             validation_id,\n\u001b[1;32m    829\u001b[0m         )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_proj_cda500_env/lib/python3.10/site-packages/hsfs/engine/python.py:1472\u001b[0m, in \u001b[0;36m_write_dataframe_kafka\u001b[0;34m(self, feature_group, dataframe, offline_write_options)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_proj_cda500_env/lib/python3.10/site-packages/hsfs/core/kafka_engine.py:76\u001b[0m, in \u001b[0;36minit_kafka_resources\u001b[0;34m(feature_group, offline_write_options, project_id)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m feature_group\u001b[38;5;241m.\u001b[39m_multi_part_insert \u001b[38;5;129;01mand\u001b[39;00m feature_group\u001b[38;5;241m.\u001b[39m_kafka_producer:\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m     71\u001b[0m         feature_group\u001b[38;5;241m.\u001b[39m_kafka_producer,\n\u001b[1;32m     72\u001b[0m         feature_group\u001b[38;5;241m.\u001b[39m_kafka_headers,\n\u001b[1;32m     73\u001b[0m         feature_group\u001b[38;5;241m.\u001b[39m_feature_writers,\n\u001b[1;32m     74\u001b[0m         feature_group\u001b[38;5;241m.\u001b[39m_writer,\n\u001b[1;32m     75\u001b[0m     )\n\u001b[0;32m---> 76\u001b[0m producer, headers, feature_writers, writer \u001b[38;5;241m=\u001b[39m \u001b[43m_init_kafka_resources\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_group\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffline_write_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproject_id\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m feature_group\u001b[38;5;241m.\u001b[39m_multi_part_insert:\n\u001b[1;32m     80\u001b[0m     feature_group\u001b[38;5;241m.\u001b[39m_kafka_producer \u001b[38;5;241m=\u001b[39m producer\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_proj_cda500_env/lib/python3.10/site-packages/hsfs/core/kafka_engine.py:95\u001b[0m, in \u001b[0;36m_init_kafka_resources\u001b[0;34m(feature_group, offline_write_options, project_id)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_init_kafka_resources\u001b[39m(\n\u001b[1;32m     88\u001b[0m     feature_group: Union[FeatureGroup, ExternalFeatureGroup],\n\u001b[1;32m     89\u001b[0m     offline_write_options: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     93\u001b[0m ]:\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;66;03m# setup kafka producer\u001b[39;00m\n\u001b[0;32m---> 95\u001b[0m     producer \u001b[38;5;241m=\u001b[39m \u001b[43minit_kafka_producer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_group\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_store_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffline_write_options\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;66;03m# setup complex feature writers\u001b[39;00m\n\u001b[1;32m     99\u001b[0m     feature_writers \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    100\u001b[0m         feature: get_encoder_func(feature_group\u001b[38;5;241m.\u001b[39m_get_feature_avro_schema(feature))\n\u001b[1;32m    101\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m feature_group\u001b[38;5;241m.\u001b[39mget_complex_features()\n\u001b[1;32m    102\u001b[0m     }\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_proj_cda500_env/lib/python3.10/site-packages/hsfs/core/kafka_engine.py:120\u001b[0m, in \u001b[0;36minit_kafka_producer\u001b[0;34m(feature_store_id, offline_write_options)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minit_kafka_producer\u001b[39m(\n\u001b[1;32m    116\u001b[0m     feature_store_id: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m    117\u001b[0m     offline_write_options: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[1;32m    118\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Producer:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;66;03m# setup kafka producer\u001b[39;00m\n\u001b[0;32m--> 120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mProducer\u001b[49m(get_kafka_config(feature_store_id, offline_write_options))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Producer' is not defined"
     ]
    }
   ],
   "source": [
    "feature_group.insert(ts_data, write_options={\"wait_for_job\": False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ff98cc-31f4-47b4-a43c-fe731760af71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_memory_mb = rides.memory_usage(deep=True).sum() / (1024 * 1024)  \n",
    "print(f\"DataFrame size: {df_memory_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be636b4b-4bd5-469d-8cc7-b09e1de29ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2277600 entries, 0 to 2277599\n",
      "Data columns (total 3 columns):\n",
      " #   Column              Dtype         \n",
      "---  ------              -----         \n",
      " 0   pickup_hour         datetime64[ns]\n",
      " 1   pickup_location_id  int16         \n",
      " 2   rides               int16         \n",
      "dtypes: datetime64[ns](1), int16(2)\n",
      "memory usage: 26.1 MB\n"
     ]
    }
   ],
   "source": [
    "ts_data.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_proj_cda500_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
